{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today I'm going to be 'actually working' on the true project for this code learning experience using generated code and then modifying it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tkinter import Tk, filedialog #using tkinter lib\n",
    "from PyPDF2 import PdfReader #using PyPDF2 as it's currently the best lib for this, must be installed first\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tkinter import Tk, filedialog\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def convert_pdf_to_text(pdf_path):\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def save_text_to_file(text, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as text_file: #you can change the utf-8 if needed\n",
    "        text_file.write(text)\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Use regular expressions to remove unwanted characters (e.g., punctuation)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    # Tokenize the text using NLTK\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "    return words\n",
    "\n",
    "def is_latin_number(word):\n",
    "    # Regex to match Latin numbers\n",
    "    latin_number_pattern = r'^(i|ii|iii|iv|v|vi|vii|viii|ix|x|xi|xii|xiii|xiv|xv|xvi|xvii|xviii|xix|xx|xxi|xxii|xxiii|xxiv|xxv|xxvi|xxvii|xxviii|xxix|xxx)$'\n",
    "    return re.match(latin_number_pattern, word) is not None\n",
    "\n",
    "def filter_words(words):\n",
    "    # Filter out single letters, numbers, and non-word strings\n",
    "    filtered_words = [word for word in words if re.match(r'^[a-zA-Z]{2,}$', word)]\n",
    "    return filtered_words\n",
    "\n",
    "def lemmatize_words(words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "def count_words(words):\n",
    "    # Use Counter to count occurrences of each word\n",
    "    word_counts = Counter(words)\n",
    "    return word_counts\n",
    "\n",
    "def save_word_counts_to_csv(word_counts, output_path):\n",
    "    # Sort word counts in decreasing order\n",
    "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Word\", \"Count\"])\n",
    "        for word, count in sorted_word_counts:\n",
    "            writer.writerow([word, count])\n",
    "\n",
    "def filter_keywords(word_counts):\n",
    "    stop_words = set(stopwords.words('english')) # Filter out stopwords and non-informative words\n",
    "    additional_stopwords = {'cf', 'sig', 'vol', 'dictionary'} #here you can filter out specific stopwords\n",
    "    stop_words.update(additional_stopwords)\n",
    "    keywords = {word: count for word, count in word_counts.items() if word not in stop_words and not is_latin_number(word)}\n",
    "    return keywords\n",
    "\n",
    "def save_keywords_to_csv(keywords, output_path):\n",
    "    # Sort keywords in decreasing order\n",
    "    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    with open(output_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerow([\"Keyword\", \"Count\"])\n",
    "        for keyword, count in sorted_keywords:\n",
    "            writer.writerow([keyword, count])\n",
    "            \n",
    "def main():\n",
    "    # Hide Tkinter root window\n",
    "    Tk().withdraw()\n",
    "    \n",
    "    # Prompt user to select a PDF file\n",
    "    pdf_path = filedialog.askopenfilename(filetypes=[(\"PDF files\", \"*.pdf\")])\n",
    "    if not pdf_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Convert PDF to text\n",
    "    text = convert_pdf_to_text(pdf_path)\n",
    "    \n",
    "    # Create output text file path\n",
    "    txt_output_path = os.path.splitext(pdf_path)[0] + \".txt\"\n",
    "    \n",
    "    # Save text to file\n",
    "    save_text_to_file(text, txt_output_path)\n",
    "    \n",
    "    print(f\"Text successfully extracted to {txt_output_path}\")\n",
    "    \n",
    "    # Read text from file\n",
    "    with open(txt_output_path, 'r', encoding='utf-8') as text_file:\n",
    "        text = text_file.read()\n",
    "    \n",
    "    # Tokenize, filter, lemmatize, and count words in text\n",
    "    words = tokenize_text(text)\n",
    "    filtered_words = filter_words(words)\n",
    "    lemmatized_words = lemmatize_words(filtered_words)\n",
    "    word_counts = count_words(lemmatized_words)\n",
    "    \n",
    "    # Create output CSV file paths\n",
    "    word_counts_output_path = os.path.splitext(txt_output_path)[0] + \"_word_counts.csv\"\n",
    "    keywords_output_path = os.path.splitext(txt_output_path)[0] + \"_keywords.csv\"\n",
    "    \n",
    "    # Save word counts to CSV\n",
    "    save_word_counts_to_csv(word_counts, word_counts_output_path)\n",
    "    \n",
    "    # Filter keywords and save to CSV\n",
    "    keywords = filter_keywords(word_counts)\n",
    "    save_keywords_to_csv(keywords, keywords_output_path)\n",
    "    \n",
    "    print(f\"Word counts successfully saved to {word_counts_output_path}\")\n",
    "    print(f\"Keywords successfully saved to {keywords_output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
